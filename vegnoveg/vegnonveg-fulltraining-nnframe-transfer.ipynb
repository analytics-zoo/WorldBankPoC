{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join, basename\n",
    "import struct\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from scipy import misc\n",
    "import datetime as dt\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline as ml_Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from math import ceil\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pylab inline\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.nn.initialization_method import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset import mnist\n",
    "from bigdl.transform.vision.image import *\n",
    "from zoo.pipeline.nnframes.nn_image_reader import *\n",
    "from zoo.pipeline.nnframes.nn_image_transformer import *\n",
    "from zoo.pipeline.nnframes.nn_classifier import *\n",
    "from zoo.common.nncontext import *\n",
    "import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def t(input_T):\n",
    "    \"\"\"\n",
    "    Helper function for building Inception layers. Transforms a list of numbers to a dictionary with ascending keys \n",
    "    and 0 appended to the front. Ignores dictionary inputs. \n",
    "    \n",
    "    :param input_T: either list or dict\n",
    "    :return: dictionary with ascending keys and 0 appended to front {0: 0, 1: realdata_1, 2: realdata_2, ...}\n",
    "    \"\"\"    \n",
    "    if type(input_T) is list:\n",
    "        # insert 0 into first index spot, such that the real data starts from index 1\n",
    "        temp = [0]\n",
    "        temp.extend(input_T)\n",
    "        return dict(enumerate(temp))\n",
    "    # if dictionary, return it back\n",
    "    return input_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_layer_v1(input_size, config, name_prefix=\"\"):\n",
    "    concat = Concat(2)\n",
    "    conv1 = Sequential()\n",
    "    conv1.add(SpatialConvolution(input_size, config[1][1], 1, 1, 1, 1)\n",
    "              .set_init_method(weight_init_method=Xavier(),bias_init_method=Zeros())\n",
    "              .set_name(name_prefix + \"1x1\"))\n",
    "    conv1.add(ReLU(True).set_name(name_prefix + \"relu_1x1\"))\n",
    "    concat.add(conv1)\n",
    "    conv3 = Sequential()\n",
    "    conv3.add(SpatialConvolution(input_size, config[2][1], 1, 1, 1, 1)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(name_prefix + \"3x3_reduce\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3_reduce\"))\n",
    "    conv3.add(SpatialConvolution(config[2][1], config[2][2], 3, 3, 1, 1, 1, 1)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(name_prefix + \"3x3\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3\"))\n",
    "    concat.add(conv3)\n",
    "    conv5 = Sequential()\n",
    "    conv5.add(SpatialConvolution(input_size, config[3][1], 1, 1, 1, 1)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(name_prefix + \"5x5_reduce\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5_reduce\"))\n",
    "    conv5.add(SpatialConvolution(config[3][1], config[3][2], 5, 5, 1, 1, 2, 2)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(name_prefix + \"5x5\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5\"))\n",
    "    concat.add(conv5)\n",
    "    pool = Sequential()\n",
    "    pool.add(SpatialMaxPooling(3, 3, 1, 1, 1, 1, to_ceil=True).set_name(name_prefix + \"pool\"))\n",
    "    pool.add(SpatialConvolution(input_size, config[4][1], 1, 1, 1, 1)\n",
    "             .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "             .set_name(name_prefix + \"pool_proj\"))\n",
    "    pool.add(ReLU(True).set_name(name_prefix + \"relu_pool_proj\"))\n",
    "    concat.add(pool).set_name(name_prefix + \"output\")\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v1_no_aux_classifier(class_num, has_dropout=True):\n",
    "    model = Sequential()\n",
    "    model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(\"conv1/7x7_s2\"))\n",
    "    model.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool1/3x3_s2\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"pool1/norm1\"))\n",
    "    model.add(SpatialConvolution(64, 64, 1, 1, 1, 1)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(\"conv2/3x3_reduce\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    model.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(\"conv2/3x3\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    model.add(inception_layer_v1(192, t([t([64]), t(\n",
    "        [96, 128]), t([16, 32]), t([32])]), \"inception_3a/\"))\n",
    "    model.add(inception_layer_v1(256, t([t([128]), t(\n",
    "        [128, 192]), t([32, 96]), t([64])]), \"inception_3b/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(inception_layer_v1(480, t([t([192]), t(\n",
    "        [96, 208]), t([16, 48]), t([64])]), \"inception_4a/\"))\n",
    "    model.add(inception_layer_v1(512, t([t([160]), t(\n",
    "        [112, 224]), t([24, 64]), t([64])]), \"inception_4b/\"))\n",
    "    model.add(inception_layer_v1(512, t([t([128]), t(\n",
    "        [128, 256]), t([24, 64]), t([64])]), \"inception_4c/\"))\n",
    "    model.add(inception_layer_v1(512, t([t([112]), t(\n",
    "        [144, 288]), t([32, 64]), t([64])]), \"inception_4d/\"))\n",
    "    model.add(inception_layer_v1(528, t([t([256]), t(\n",
    "        [160, 320]), t([32, 128]), t([128])]), \"inception_4e/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(inception_layer_v1(832, t([t([256]), t(\n",
    "        [160, 320]), t([32, 128]), t([128])]), \"inception_5a/\"))\n",
    "    model.add(inception_layer_v1(832, t([t([384]), t(\n",
    "        [192, 384]), t([48, 128]), t([128])]), \"inception_5b/\"))\n",
    "    model.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    if has_dropout:\n",
    "        model.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    model.add(View([1024], num_input_dims=3))\n",
    "    model.add(Linear(1024, class_num)\n",
    "              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n",
    "              .set_name(\"loss3/classifier\"))\n",
    "    model.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "    model.reset()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inception_v1(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False).set_name(\"conv1/7x7_s2\"))\n",
    "    model.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool1/3x3_s2\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"pool1/norm1\"))\n",
    "    model.add(SpatialConvolution(64, 64, 1, 1, 1, 1).set_name(\"conv2/3x3_reduce\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    model.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1).set_name(\"conv2/3x3\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    model.add(inception_layer_v1(192, scala_T([scala_T([64]), scala_T(\n",
    "         [96, 128]), scala_T([16, 32]), scala_T([32])]), \"inception_3a/\"))\n",
    "    model.add(inception_layer_v1(256, scala_T([scala_T([128]), scala_T(\n",
    "         [128, 192]), scala_T([32, 96]), scala_T([64])]), \"inception_3b/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(inception_layer_v1(480, scala_T([scala_T([192]), scala_T(\n",
    "         [96, 208]), scala_T([16, 48]), scala_T([64])]), \"inception_4a/\"))\n",
    "    model.add(inception_layer_v1(512, scala_T([scala_T([160]), scala_T(\n",
    "         [112, 224]), scala_T([24, 64]), scala_T([64])]), \"inception_4b/\"))\n",
    "    model.add(inception_layer_v1(512, scala_T([scala_T([128]), scala_T(\n",
    "         [128, 256]), scala_T([24, 64]), scala_T([64])]), \"inception_4c/\"))\n",
    "    model.add(inception_layer_v1(512, scala_T([scala_T([112]), scala_T(\n",
    "         [144, 288]), scala_T([32, 64]), scala_T([64])]), \"inception_4d/\"))\n",
    "    model.add(inception_layer_v1(528, scala_T([scala_T([256]), scala_T(\n",
    "         [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_4e/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(inception_layer_v1(832, scala_T([scala_T([256]), scala_T(\n",
    "         [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_5a/\"))\n",
    "    model.add(inception_layer_v1(832, scala_T([scala_T([384]), scala_T(\n",
    "         [192, 384]), scala_T([48, 128]), scala_T([128])]), \"inception_5b/\"))\n",
    "    model.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    model.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    model.add(View([1024], num_input_dims=3))\n",
    "    model.add(Linear(1024, class_num).set_name(\"loss3/classifier\"))\n",
    "    model.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "    model.reset()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the images from Amazon s3\n\nMake sure you have AWS command line interface to recursively download all images in s3 folder. You can set up aws cli from this link: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from os import path\n",
    "MODEL_ROOT = \"/mnt/nobigdl/vegnonveg/python/inception_v1/models/\"\n",
    "checkpoint_path = path.join(MODEL_ROOT, \"checkpoints\")\n",
    "\n",
    "# if not path.isdir(local_folder):\n",
    "#   os.system('aws s3 cp --recursive s3://vegnonveg/vegnonveg-fewsamples %s' % local_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images to parquet fileand load to Spark as Image dataframe\n",
    "\n",
    "save data to parquet files and load to spark. Add label to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/mnt/nobigdl/vegnonveg/python/inception_v1/sample_images/\"\n",
    "sample_path = DATA_ROOT + 'vegnonveg-fewsamples/'\n",
    "label_path = DATA_ROOT + 'vegnonveg-samples_labels.csv'\n",
    "parquet_path = DATA_ROOT + 'sample_parquet/'\n",
    "dbutils.fs.rm(parquet_path, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intializa bigdl\n",
    "init_engine()\n",
    "redire_spark_logs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only runs at the first time to generate parquet files\n",
    "image_frame = NNImageReader.readImages(sample_path, sc, minParitions=32)\n",
    "# save dataframe to parquet files\n",
    "# image_frame.write.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parquet file into spark cluster\n",
    "import time\n",
    "start = time.time()\n",
    "# image_raw_DF = sqlContext.read.parquet(parquet_path)\n",
    "end = time.time()\n",
    "print(\"Load data time is: \" + str(end-start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label dataframe\n",
    "label_raw_DF = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "    .load(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image data dataframe\n",
    "get_name = udf(lambda row: row[0].split(\"/\")[-1], StringType())\n",
    "imageDF = image_frame.withColumn(\"image_name\", get_name(\"image\"))\n",
    "# imageDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col, rand\n",
    "# image dataframe join with labels\n",
    "dataDF = imageDF.join(label_raw_DF, imageDF.image_name==label_raw_DF.obs_uid, \"inner\").select(\"image\", \"image_name\", \"item_name\")\n",
    "# only use samples whose label count > 100\n",
    "items = dataDF.groupBy(\"item_name\").count().filter(\"count > 100\").select(\"item_name\")\n",
    "indexer = StringIndexer(inputCol=\"item_name\", outputCol=\"label\")\n",
    "labels = indexer.fit(items).transform(items).withColumn(\"label\", (col(\"label\") + 1).cast(\"float\"))\n",
    "filteredDF = dataDF.join(labels, \"item_name\", \"inner\").select(\"image\", \"image_name\", \"label\").orderBy(rand())\n",
    "n_classes = labels.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Train/Test Split and preprocessing\nSplit Train/Test split with some ratio and preprocess images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filteredDF.randomSplit([0.9, 0.1], seed=10)\n",
    "train_image = data[0]\n",
    "val_image = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "train_transformer = NNImageTransformer(\n",
    "    Pipeline([Resize(256, 256), CenterCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "              ChannelNormalize(123.0, 117.0, 104.0, 1.0, 1.0, 1.0),\n",
    "              MatToTensor()])\n",
    ").setInputCol(\"image\").setOutputCol(\"features\")\n",
    "\n",
    "train_data = train_transformer.transform(train_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = train_image.count()\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transformer = NNImageTransformer(\n",
    "    Pipeline([Resize(256,256),\n",
    "              CenterCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "              ChannelNormalize(123.0, 117.0, 104.0, 1.0, 1.0, 1.0),\n",
    "              MatToTensor(to_rgb=True)]\n",
    "            )\n",
    ").setInputCol(\"image\").setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = val_transformer.transform(val_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"dbfs:\" + MODEL_ROOT + \"bvlc_googlenet.caffemodel\"\n",
    "def_path = \"dbfs:\" + MODEL_ROOT + \"deploy_transfer.prototxt\"\n",
    "model = Model.load_caffe_model(def_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.2\n",
    "# parameters for \n",
    "batch_size = 64 #depends on dataset\n",
    "no_epochs = 40 #stop when validation accuracy doesn't improve anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "preTrainedNNModel = NNModel(model, [3,224,224]).setPredictionCol(\"embedding\").setBatchSize(batch_size)\n",
    "lrModel = Sequential().add(View([1024])).add(Linear(1024, n_classes)).add(LogSoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ClassNLLCriterion()\n",
    "iterations = int(ceil(float(train_size) / batch_size))\n",
    "optim = SGD(learningrate=learning_rate, learningrate_decay=0.0,\n",
    "                    momentum=0.9, dampening=0.0, nesterov=False,\n",
    "                    leaningrate_schedule=Poly(0.5, iterations))\n",
    "classifier = NNClassifier(lrModel, criterion, [1024])\\\n",
    "    .setBatchSize(batch_size)\\\n",
    "    .setMaxEpoch(no_epochs)\\\n",
    "    .setLearningRate(learning_rate)\\\n",
    "    .setFeaturesCol(\"embedding\")\n",
    "pipeline = ml_Pipeline(stages=[preTrainedNNModel, classifier])\n",
    "start = time.time()\n",
    "trained_model = pipeline.fit(train_data)\n",
    "end = time.time()\n",
    "print(\"Optimization Done.\")\n",
    "print(\"Training time is: %s seconds\" % str(end-start))\n",
    "# + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput = train_size * no_epochs / (end - start)\n",
    "print(\"Average throughput is: %s\" % str(throughput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "# predict_model = trained_model.setBatchSize(batch_size)\n",
    "predictionDF = trained_model.transform(test_data)\n",
    "predictionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preds = 1\n",
    "preds = predictionDF.select(\"label\", \"prediction\").take(num_preds)\n",
    "for idx in range(num_preds):\n",
    "#    true_label = str(map_to_label(map_groundtruth_label(truth[idx].label)))\n",
    "    true_label = preds[idx][0]\n",
    "    pred_label = preds[idx][1]\n",
    "    print(str(idx + 1) +')'+ 'Ground Truth label: '+ str(true_label))\n",
    "    print(str(idx + 1) + ')'+ 'Predicted label: '+ str(pred_label))\n",
    "    print(\"correct\" if true_label == pred_label else \"wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Measure Test Accuracy w/Test Set\n",
    "'''\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictionDF)\n",
    "print(\"Accuracy = %g \" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "name": "vegnonveg-fulltraining-nnframe-transfer",
  "notebookId": 2.948019328317446E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
